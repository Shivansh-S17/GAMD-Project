# -*- coding: utf-8 -*-
"""Untitled1.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/144dX27r8U3vMeyTgeoCNPYfJNSVmN7_7
"""

!pip install robust-laplacian numpy scipy

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import h5py
import os
from torch.utils.data import Dataset, DataLoader
import time
from scipy.sparse import csr_matrix
from scipy.sparse.linalg import svds
from scipy.optimize import minimize
from scipy.spatial import cKDTree
import warnings
warnings.filterwarnings('ignore')

# Define constants
D = 3
M = 1536
k_init = 256
k_add = 256
k_opt = 4

def gp_pcs_simplify(point_cloud, num_points):
    """
    GPU-accelerated Farthest Point Sampling maintaining original logic
    """
    if isinstance(point_cloud, np.ndarray):
        point_cloud = torch.tensor(point_cloud, dtype=torch.float32)

    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    point_cloud = point_cloud.to(device)

    n_points = point_cloud.shape[0]
    simplified_cloud = torch.zeros((num_points, point_cloud.shape[1]), device=device)

    # Random initial point
    initial_idx = torch.randint(0, n_points, (1,), device=device)
    simplified_cloud[0] = point_cloud[initial_idx]

    # Keep track of remaining points
    remaining_mask = torch.ones(n_points, dtype=torch.bool, device=device)
    remaining_mask[initial_idx] = False

    for i in range(1, num_points):
        if not remaining_mask.any():
            break

        remaining_points = point_cloud[remaining_mask]

        # Compute distances to all selected points efficiently on GPU
        dists_to_selected = torch.cdist(remaining_points.unsqueeze(0),
                                       simplified_cloud[:i].unsqueeze(0)).squeeze(0)
        min_dists = torch.min(dists_to_selected, dim=1)[0]

        # Find farthest point
        farthest_local_idx = torch.argmax(min_dists)
        remaining_indices = torch.where(remaining_mask)[0]
        farthest_global_idx = remaining_indices[farthest_local_idx]

        simplified_cloud[i] = point_cloud[farthest_global_idx]
        remaining_mask[farthest_global_idx] = False

    return simplified_cloud.cpu().numpy()

class GaussianProcess:
    """GPU-accelerated Gaussian Process maintaining original functionality"""
    def __init__(self, kernel, noise_var=1e-6):
        self.kernel = kernel
        self.sigma_y2 = noise_var**2
        self.is_fit = False
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def fit(self, X, y):
        # Convert to tensors and move to GPU
        self.X_train = torch.tensor(X, dtype=torch.float32, device=self.device)
        self.y_train = torch.tensor(y, dtype=torch.float32, device=self.device)

        # Compute kernel matrix on GPU
        K = self.kernel(self.X_train.cpu().numpy(), self.X_train.cpu().numpy())
        K = torch.tensor(K, dtype=torch.float32, device=self.device)
        K += self.sigma_y2 * torch.eye(len(X), device=self.device)
        K += 1e-6 * torch.eye(len(X), device=self.device)  # Jitter for stability

        # Use GPU Cholesky decomposition for better numerical stability
        try:
            self.L = torch.linalg.cholesky(K)
            # Solve using GPU operations
            temp = torch.linalg.solve_triangular(self.L, self.y_train.unsqueeze(1), upper=False)
            self.alpha = torch.linalg.solve_triangular(self.L.T, temp, upper=True).squeeze()
        except Exception as e:
            # print(f"Cholesky failed, using regularized solve: {e}")
            K += 1e-3 * torch.eye(len(X), device=self.device)
            self.alpha = torch.linalg.solve(K, self.y_train)
            self.L = torch.linalg.cholesky(K)

        self.is_fit = True

    def predict(self, X_test):
        if not self.is_fit:
            raise ValueError("Call fit(X, y) before predict().")

        X_test = np.atleast_2d(X_test)
        X_train_np = self.X_train.cpu().numpy()

        K_test = self.kernel(X_test, X_train_np)
        K_test = torch.tensor(K_test, dtype=torch.float32, device=self.device)

        # Mean prediction using GPU
        mean = torch.mv(K_test, self.alpha)

        # Variance computation using GPU - this is the key optimization
        # Instead of solving the full system, use the precomputed Cholesky
        v = torch.linalg.solve_triangular(self.L, K_test.T, upper=False)

        K_test_test = self.kernel(X_test, X_test)
        K_test_test = torch.tensor(np.diag(K_test_test), dtype=torch.float32, device=self.device)

        var = K_test_test - torch.sum(v * v, dim=0)
        var = torch.clamp(var, min=1e-6)

        return mean.cpu().numpy(), var.cpu().numpy()

    def log_marginal_likelihood(self, theta):
        if not hasattr(self, 'X_train') or not hasattr(self, 'y_train'):
            raise ValueError("Call fit(X, y) before computing the log marginal likelihood.")

        self.kernel.theta = theta
        X_train_np = self.X_train.cpu().numpy()
        K = self.kernel(X_train_np, X_train_np)
        K = torch.tensor(K, dtype=torch.float32, device=self.device)
        K += self.sigma_y2 * torch.eye(len(X_train_np), device=self.device)
        K += 1e-3 * torch.eye(len(X_train_np), device=self.device)

        try:
            L = torch.linalg.cholesky(K)
            temp = torch.linalg.solve_triangular(L, self.y_train.unsqueeze(1), upper=False)
            alpha = torch.linalg.solve_triangular(L.T, temp, upper=True).squeeze()
            log_det = 2 * torch.sum(torch.log(torch.diag(L)))
        except:
            return -np.inf

        log_likelihood = (
            -0.5 * torch.dot(self.y_train, alpha)
            - 0.5 * log_det
            - len(self.y_train)/2 * np.log(2*np.pi)
        )

        if not torch.isfinite(log_likelihood):
            return -np.inf

        return log_likelihood.item()

class RiemannianKernel:
    """Optimized Riemannian Kernel with GPU acceleration where possible"""
    def __init__(self, sigma_y=0.73, kappa=2.32, nu=-1.72, eigenvalues=None, eigenfunctions=None, vertices=None):
        self.sigma_y = sigma_y
        self.kappa = kappa
        self.nu = nu
        self.theta = (sigma_y, kappa, nu)
        self.Cv = 1.0
        self.eigenvalues = eigenvalues
        self.eigenfunctions = eigenfunctions
        self.vertices = vertices
        self.device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    def __call__(self, X1, X2):
        if self.eigenvalues is None or self.eigenfunctions is None:
            raise ValueError("Eigenvalues and eigenfunctions must be provided.")

        sigma_y, kappa, nu = self.theta
        n1, n2 = len(X1), len(X2)

        # Precompute eigenfunction values - this is where we optimize
        φn_X1_all = evaluate_eigenfunctions_at_points_gpu(self.eigenfunctions, self.vertices, X1)
        φn_X2_all = evaluate_eigenfunctions_at_points_gpu(self.eigenfunctions, self.vertices, X2)

        # Move to GPU for computation
        eigenvalues_gpu = torch.tensor(self.eigenvalues, dtype=torch.float32, device=self.device)
        phi_X1_gpu = torch.tensor(φn_X1_all, dtype=torch.float32, device=self.device)
        phi_X2_gpu = torch.tensor(φn_X2_all, dtype=torch.float32, device=self.device)

        # Vectorized kernel computation on GPU
        coeffs = ((2 * nu / (kappa * 2)) + eigenvalues_gpu) ** (-nu - 1.5)

        # Efficient outer product computation
        K = torch.sum(coeffs.unsqueeze(1).unsqueeze(2) *
                     torch.bmm(phi_X1_gpu.unsqueeze(2), phi_X2_gpu.unsqueeze(1)), dim=0)

        return (sigma_y ** 2 / self.Cv) * K.cpu().numpy()

def point_cloud_laplacian_gpu(vertices, n_neighbors=16):
    """GPU-accelerated Laplacian computation"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    vertices_gpu = torch.tensor(vertices, dtype=torch.float32, device=device)
    n_points = len(vertices)

    # Compute pairwise distances on GPU
    dists = torch.cdist(vertices_gpu, vertices_gpu)

    # Find k-nearest neighbors
    _, neighbor_indices = torch.topk(dists, n_neighbors + 1, dim=1, largest=False)
    neighbor_indices = neighbor_indices[:, 1:]  # Exclude self

    # Build adjacency matrix
    W = torch.zeros((n_points, n_points), device=device)
    for i in range(n_points):
        neighbors = neighbor_indices[i]
        W[i, neighbors] = 1.0

    # Symmetrize
    W = (W + W.T) / 2

    # Degree matrix
    D = torch.diag(torch.sum(W, dim=1))

    # Laplacian
    L = D - W

    return L.cpu().numpy(), D.cpu().numpy()

def estimate_eigenvalues_and_eigenfunctions(vertices, num_eigenvalues=1):
    """Optimized eigenvalue computation"""
    num_points = len(vertices)
    k_neighbors = min(16, num_points - 1)

    L, M = point_cloud_laplacian_gpu(vertices, n_neighbors=k_neighbors)
    L = csr_matrix(L)

    try:
        U, s, Vt = svds(L, k=min(num_eigenvalues, len(L.toarray()) - 1))
    except Exception as e:
        print(f"SVD computation failed: {e}")
        return None, None

    return s, U.T

def evaluate_eigenfunctions_at_points_gpu(eigenfunctions, vertices, points):
    """GPU-accelerated eigenfunction evaluation"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

    # Use GPU for distance computation if arrays are large enough
    if len(vertices) > 1000 and len(points) > 100:
        vertices_gpu = torch.tensor(vertices, dtype=torch.float32, device=device)
        points_gpu = torch.tensor(points, dtype=torch.float32, device=device)

        # Compute distances on GPU
        dists = torch.cdist(points_gpu, vertices_gpu)
        _, idx = torch.min(dists, dim=1)
        idx = idx.cpu().numpy()
    else:
        # Use scipy for smaller arrays
        from scipy.spatial import cKDTree
        tree = cKDTree(vertices)
        _, idx = tree.query(points)

    values = eigenfunctions[:, idx]
    return values

def compute_surface_variation_gpu(points):
    """GPU-accelerated surface variation computation maintaining original logic"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print("Computing Surface Variation (SV) with GPU acceleration")

    points_array = np.ascontiguousarray(np.transpose(points, (0, 2, 1)))
    k_neighbors = 35
    num_samples, num_points, _ = points_array.shape
    variations = np.zeros((num_samples, num_points))

    # Process in batches to manage GPU memory
    batch_size = min(100, num_samples)

    for batch_start in range(0, num_samples, batch_size):
        batch_end = min(batch_start + batch_size, num_samples)
        batch_samples = points_array[batch_start:batch_end]

        # Convert batch to GPU tensor
        batch_gpu = torch.tensor(batch_samples, dtype=torch.float32, device=device)
        batch_size_actual = batch_gpu.shape[0]

        batch_variations = torch.zeros((batch_size_actual, num_points), device=device)

        for i in range(batch_size_actual):
            sample = batch_gpu[i]  # Shape (num_points, 3)

            # Compute pairwise distances on GPU
            dists = torch.cdist(sample.unsqueeze(0), sample.unsqueeze(0)).squeeze(0)

            # Find k nearest neighbors
            _, neighbor_idx = torch.topk(dists, k_neighbors + 1, dim=1, largest=False)
            neighbor_idx = neighbor_idx[:, 1:]  # Exclude self

            # Get neighbor coordinates
            neighbors = sample[neighbor_idx]  # Shape (num_points, k_neighbors, 3)

            # Vectorized centroid and covariance computation
            centroids = neighbors.mean(dim=1, keepdim=True)
            centered = neighbors - centroids

            # Batch covariance matrix computation
            cov_mats = torch.bmm(centered.transpose(1, 2), centered) / (k_neighbors - 1)

            # Batch eigenvalue computation
            eigenvals = torch.linalg.eigvals(cov_mats).real
            eigenvals = torch.sort(eigenvals, dim=1)[0]

            # Surface variation computation
            min_eigen = eigenvals[:, 0]
            total_eigen = eigenvals.sum(dim=1)
            batch_variations[i] = torch.where(total_eigen != 0,
                                            min_eigen / total_eigen,
                                            torch.zeros_like(min_eigen))

        # Copy back to CPU
        variations[batch_start:batch_end] = batch_variations.cpu().numpy()

        if batch_start % (batch_size * 10) == 0:
            print(f"Processed {batch_start}/{num_samples} samples")

    print("Done computing surface variations")
    return variations

def optimize_gp_hyperparameters_gpu(gp, X_opt, y_opt):
    """GPU-accelerated hyperparameter optimization with proper parameter updating"""
    def neg_log_likelihood(theta):
        try:
            # Temporarily set the parameters
            old_theta = gp.kernel.theta
            gp.kernel.theta = theta

            # Compute negative log likelihood
            nll = -gp.log_marginal_likelihood(theta)

            # Restore old parameters in case of failure
            if not np.isfinite(nll):
                gp.kernel.theta = old_theta
                return 1e6

            return nll
        except Exception as e:
            print(f"Error in likelihood computation: {e}")
            return 1e6

    # Get initial parameters from current kernel
    initial_theta = list(gp.kernel.theta)
    print(f"Initial hyperparameters: {initial_theta}")

    # Define reasonable bounds for each parameter
    bounds = [
        (0.1, 2.0),   # sigma_y: signal variance
        (0.5, 5.0),   # kappa: length scale parameter
        (0.1, 3.0)    # nu: smoothness parameter
    ]

    # Try multiple optimization attempts with different starting points
    best_result = None
    best_score = np.inf

    # Starting points: current + some variations
    starting_points = [
        initial_theta,
        [0.3, 1.8, 1.2],  # Original default
        [0.5, 2.5, 1.5],  # Alternative 1
        [1.0, 1.0, 2.0],  # Alternative 2
    ]

    for i, start_point in enumerate(starting_points):
        try:
            print(f"Optimization attempt {i+1} with starting point: {start_point}")

            result = minimize(
                neg_log_likelihood,
                x0=start_point,
                method="L-BFGS-B",
                bounds=bounds,
                options={
                    "maxiter": 100,
                    "ftol": 1e-6,
                    "gtol": 1e-6,
                    "disp": False
                }
            )

            if result.success and result.fun < best_score:
                best_result = result
                best_score = result.fun
                print(f"  Success! Score: {result.fun:.6f}, Params: {result.x}")
            else:
                print(f"  Failed or worse score: {result.fun:.6f}")

        except Exception as e:
            print(f"  Optimization attempt {i+1} failed: {e}")
            continue

    if best_result is not None and best_result.success:
        optimal_params = best_result.x
        print(f"Best optimized hyperparameters: {optimal_params}")
        print(f"Improvement in likelihood: {best_score:.6f}")

        # Verify the optimized parameters are reasonable
        if all(bounds[i][0] <= optimal_params[i] <= bounds[i][1] for i in range(len(optimal_params))):
            return optimal_params
        else:
            print("Optimized parameters outside bounds, using defaults")
            return initial_theta
    else:
        print("All optimization attempts failed, using initial parameters")
        return initial_theta

def greedy_subset_of_data_gpu(points, surface_variations, M, k_init, k_add, k_opt=32):
    """GPU-accelerated greedy selection with FIXED hyperparameter optimization"""
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    n, num_points, _ = points.shape

    # Initialize Active Sets and Remainder Indices
    remainder_indices = [np.arange(num_points) for _ in range(n)]
    active_set = [np.zeros((M, 3), dtype=np.float32) for _ in range(n)]
    active_count = np.full(n, k_init, dtype=int)

    print("Initializing active sets with GPU-accelerated FPS...")
    for i in range(n):
        initial_points = gp_pcs_simplify(points[i], k_init)
        active_set[i][:k_init] = initial_points

        # GPU-accelerated nearest neighbor search for large point clouds
        if num_points > 1000:
            points_gpu = torch.tensor(points[i], dtype=torch.float32, device=device)
            initial_gpu = torch.tensor(initial_points, dtype=torch.float32, device=device)
            dists = torch.cdist(initial_gpu, points_gpu)
            _, active_idx = torch.min(dists, dim=0)
            active_idx = active_idx.cpu().numpy()
        else:
            from scipy.spatial import cKDTree
            kdtree = cKDTree(points[i])
            _, active_idx = kdtree.query(initial_points, k=1)

        remainder_indices[i] = np.setdiff1d(remainder_indices[i], active_idx)

    # Estimate GP Kernel (Shared across all samples)
    print("Estimating GP kernels...")
    P_opt = np.array([gp_pcs_simplify(points[i], k_opt) for i in range(n)])
    eigenvalues_list, eigenfunctions_list = [], []

    for i in range(n):
        vertices = P_opt[i]
        eigenvalues, eigenfunctions = estimate_eigenvalues_and_eigenfunctions(vertices)
        if eigenvalues is not None and eigenfunctions is not None:
            eigenvalues_list.append(eigenvalues)
            eigenfunctions_list.append(eigenfunctions)

    if not eigenvalues_list:
        raise ValueError("Failed to compute eigenvalues/eigenfunctions")

    mean_eigenvalues = np.mean(np.array(eigenvalues_list), axis=0)
    mean_eigenfunctions = np.mean(np.array(eigenfunctions_list), axis=0)

    # Create GP with initial kernel parameters
    kernel = RiemannianKernel(
        eigenvalues=mean_eigenvalues,
        eigenfunctions=mean_eigenfunctions,
        vertices=P_opt[0]
    )

    gp = GaussianProcess(kernel)

    # FIXED: Properly optimize hyperparameters and apply them
    print("Optimizing GP hyperparameters...")
    try:
        # Fit GP with initial parameters to prepare for optimization
        sample_points = P_opt[0]
        sample_vars = surface_variations[0][:len(sample_points)]

        print(f"Fitting GP with {len(sample_points)} points for hyperparameter optimization...")
        gp.fit(sample_points, sample_vars)

        # Optimize hyperparameters
        optimal_theta = optimize_gp_hyperparameters_gpu(gp, sample_points, sample_vars)

        # CRUCIAL FIX: Actually update the kernel parameters
        gp.kernel.theta = optimal_theta
        gp.kernel.sigma_y, gp.kernel.kappa, gp.kernel.nu = optimal_theta

        print(f"Applied optimized hyperparameters: sigma_y={optimal_theta[0]:.4f}, kappa={optimal_theta[1]:.4f}, nu={optimal_theta[2]:.4f}")

        # Re-fit with optimized parameters
        gp.fit(sample_points, sample_vars)
        print("GP re-fitted with optimized hyperparameters")

    except Exception as e:
        print(f"Hyperparameter optimization failed: {e}")
        print("Using default hyperparameters")
        default_theta = [0.73, 2.32, -1.72]
        gp.kernel.theta = default_theta
        gp.kernel.sigma_y, gp.kernel.kappa, gp.kernel.nu = default_theta

    # Greedy Selection Loop with GPU acceleration
    print("Starting greedy selection...")
    completed = np.zeros(n, dtype=bool)
    batch_size = 4  # Smaller batches for memory management

    iteration = 0
    max_iterations = (M - k_init) // k_add + 1

    while not completed.all() and iteration < max_iterations:
        print(f"Iteration {iteration + 1}/{max_iterations}")

        for batch_start in range(0, n, batch_size):
            batch_indices = list(range(batch_start, min(batch_start + batch_size, n)))
            active_points, active_vars = [], []

            for i in batch_indices:
                if completed[i]:
                    continue
                active_points.append(active_set[i][:active_count[i]])
                active_vars.append(surface_variations[i][:active_count[i]])

            if not active_points:
                continue

            # Fit GP to current batch
            try:
                combined_points = np.concatenate(active_points)
                combined_vars = np.concatenate(active_vars)
                gp.fit(combined_points, combined_vars)
                # print(f"  GP fitted with {len(combined_points)} points using optimized hyperparameters")
            except Exception as e:
                print(f"GP fit failed: {e}, skipping batch")
                continue

            for i in batch_indices:
                if completed[i] or len(remainder_indices[i]) == 0:
                    continue

                current_indices = remainder_indices[i]
                current_points = points[i][current_indices]
                current_vars = surface_variations[i][current_indices]

                try:
                    mean, var = gp.predict(current_points)
                    selection = np.sqrt(var) + np.abs(mean - current_vars)
                except Exception as e:
                    print(f"GP prediction failed: {e}, using random selection")
                    selection = np.random.rand(len(current_indices))

                valid_k = min(k_add, len(selection))
                if valid_k == 0:
                    continue

                selected_local = np.argpartition(selection, -valid_k)[-valid_k:]
                selected_global = current_indices[selected_local]
                add_count = min(valid_k, M - active_count[i])

                if add_count == 0:
                    completed[i] = True
                    continue

                # Add new points
                active_set[i][active_count[i]:active_count[i] + add_count] = points[i][selected_global[:add_count]]
                active_count[i] += add_count

                # Update remainder
                remainder_indices[i] = np.setdiff1d(remainder_indices[i], selected_global[:add_count])

                if active_count[i] >= M:
                    completed[i] = True

        iteration += 1

    print("Simplification complete.")
    return np.array([a[:M] for a in active_set])

# PointNet classes remain the same as they're already optimized
class PointNetFeature(nn.Module):
    def __init__(self, global_feature=True, feature_transform=False):
        super(PointNetFeature, self).__init__()
        self.global_feature = global_feature
        self.feature_transform = feature_transform

        self.conv1 = torch.nn.Conv1d(3, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 128, 1)
        self.conv3 = torch.nn.Conv1d(128, 1024, 1)

        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)

        if self.feature_transform:
            self.fstn = FeatureTransformNet()

    def forward(self, x):
        x = F.relu(self.bn1(self.conv1(x)))

        if self.feature_transform:
            trans_feat = self.fstn(x)
            x = x.transpose(2, 1)
            x = torch.bmm(x, trans_feat)
            x = x.transpose(2, 1)

        x = F.relu(self.bn2(self.conv2(x)))
        x = self.bn3(self.conv3(x))
        x = torch.max(x, 2, keepdim=True)[0]

        return x.view(-1, 1024)

class PointNetClassification(nn.Module):
    def __init__(self, num_classes=40, feature_transform=False):
        super(PointNetClassification, self).__init__()

        self.feature_extraction = PointNetFeature(
            global_feature=True,
            feature_transform=feature_transform
        )

        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, num_classes)

        self.bn1 = nn.BatchNorm1d(512)
        self.bn2 = nn.BatchNorm1d(256)
        self.dropout = nn.Dropout(p=0.4)

    def forward(self, x):
        x = self.feature_extraction(x)
        x = F.relu(self.bn1(self.fc1(x)))
        x = F.relu(self.bn2(self.fc2(x)))
        x = self.dropout(x)
        x = self.fc3(x)
        return F.log_softmax(x, dim=-1)

class FeatureTransformNet(nn.Module):
    def __init__(self, k=64):
        super(FeatureTransformNet, self).__init__()

        self.conv1 = torch.nn.Conv1d(k, 64, 1)
        self.conv2 = torch.nn.Conv1d(64, 128, 1)
        self.conv3 = torch.nn.Conv1d(128, 1024, 1)

        self.fc1 = nn.Linear(1024, 512)
        self.fc2 = nn.Linear(512, 256)
        self.fc3 = nn.Linear(256, k*k)

        self.bn1 = nn.BatchNorm1d(64)
        self.bn2 = nn.BatchNorm1d(128)
        self.bn3 = nn.BatchNorm1d(1024)
        self.bn4 = nn.BatchNorm1d(512)
        self.bn5 = nn.BatchNorm1d(256)

    def forward(self, x):
        batch_size = x.size(0)
        k = int(np.sqrt(self.fc3.out_features))

        x = F.relu(self.bn1(self.conv1(x)))
        x = F.relu(self.bn2(self.conv2(x)))
        x = F.relu(self.bn3(self.conv3(x)))

        x = torch.max(x, 2, keepdim=True)[0]

        x = F.relu(self.bn4(self.fc1(x.squeeze(-1))))
        x = F.relu(self.bn5(self.fc2(x)))
        x = self.fc3(x)

        iden = torch.eye(k).view(1, -1).repeat(batch_size, 1)
        if x.is_cuda:
            iden = iden.cuda()
        x = x + iden
        x = x.view(batch_size, k, k)

        return x

class ModelNet40Dataset(torch.utils.data.Dataset):
    def __init__(self, directory, train=True):
        self.points1 = []
        self.labels = []

        h5_files = []
        for f in os.listdir(directory):
            if f.endswith('.h5') and ('train' in f.lower()):
                h5_files.append(f)

        print(f"Found {len(h5_files)} training files: {h5_files}")

        for filename in h5_files:
            filepath = os.path.join(directory, filename)
            with h5py.File(filepath, 'r') as f:
                points = f['data'][:]
                labels = f['label'][:]

                self.points1.append(points)
                self.labels.append(labels)

        self.points1 = np.concatenate(self.points1, axis=0)
        self.labels = np.concatenate(self.labels, axis=0).squeeze()

        self.points1 = torch.from_numpy(self.points1.transpose(0, 2, 1)).float()
        self.labels = torch.from_numpy(self.labels).long()

        self.points1 = self.points1.numpy()
        reshaped_points = np.transpose(self.points1, (0, 2, 1))
        print("Reshaped points shape", reshaped_points.shape)

        # GPU-accelerated surface variation computation
        surface_variations = compute_surface_variation_gpu(self.points1)

        # GPU-accelerated greedy selection
        self.points = greedy_subset_of_data_gpu(reshaped_points, surface_variations, M, k_init, k_add)

        print("Simplified Point Cloud Shape:", self.points.shape)
        print(f"Loaded {len(self.labels)} training samples")

    def __len__(self):
        return len(self.labels)

    def __getitem__(self, idx):
        return self.points[idx].astype(np.float32), self.labels[idx]

def train(model, train_loader, optimizer, criterion, device):
    model.train()
    total_loss = 0
    correct = 0
    total_samples = 0

    for points, labels in train_loader:
        points, labels = points.to(device).float(), labels.to(device)
        points = points.permute(0, 2, 1)

        optimizer.zero_grad()
        outputs = model(points)
        loss = criterion(outputs, labels)

        loss.backward()
        optimizer.step()

        _, predicted = torch.max(outputs.data, 1)
        total_samples += labels.size(0)
        correct += (predicted == labels).sum().item()
        total_loss += loss.item()

    return total_loss / len(train_loader), correct / total_samples

def validate(model, val_loader, criterion, device):
    model.eval()
    total_loss = 0
    correct = 0
    total_samples = 0

    with torch.no_grad():
        for points, labels in val_loader:
            points, labels = points.to(device).float(), labels.to(device)
            points = points.permute(0, 2, 1)

            outputs = model(points)
            loss = criterion(outputs, labels)

            _, predicted = torch.max(outputs.data, 1)
            total_samples += labels.size(0)
            correct += (predicted == labels).sum().item()
            total_loss += loss.item()

    return total_loss / len(val_loader), correct / total_samples

def main():
    # Hyperparameters
    batch_size = 16
    learning_rate = 0.001
    num_epochs = 75
    num_classes = 40

    # Device configuration
    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')
    print(f"Using device: {device}")

    # Dataset paths (adjust as needed)
    dataset_paths = ["/content/sample_data/Modelnet40"]

    # Find a valid dataset path
    dataset_path = None
    for path in dataset_paths:
        if os.path.exists(path):
            dataset_path = path
            print(f"Using dataset path: {dataset_path}")
            break

    if not dataset_path:
        raise ValueError("Could not find ModelNet40 dataset")

    # Create dataset (only training data)
    print("Loading and processing dataset...")
    train_dataset = ModelNet40Dataset(dataset_path, train=True)

    # Create data loader
    train_loader = torch.utils.data.DataLoader(
        train_dataset,
        batch_size=batch_size,
        shuffle=True,
        num_workers=0
    )

    # Initialize model
    print("Initializing PointNet model...")
    model = PointNetClassification(num_classes=num_classes).to(device)

    # Print model parameters
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"Total parameters: {total_params:,}")
    print(f"Trainable parameters: {trainable_params:,}")

    # Loss and optimizer
    criterion = nn.CrossEntropyLoss(label_smoothing=0.1)
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)

    # Learning rate scheduler
    scheduler = optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs, eta_min=1e-5)

    # Training tracking
    best_train_acc = 0.0
    train_losses = []
    train_accuracies = []

    # Training loop
    print("Starting training...")
    print("-" * 60)

    for epoch in range(num_epochs):
        epoch_start_time = time.time()

        # Training
        train_loss, train_acc = train(model, train_loader, optimizer, criterion, device)

        # Track metrics
        train_losses.append(train_loss)
        train_accuracies.append(train_acc)

        # Step the learning rate scheduler
        scheduler.step()
        current_lr = scheduler.get_last_lr()[0]

        # Calculate epoch time
        epoch_time = time.time() - epoch_start_time

        # Save best model
        if train_acc > best_train_acc:
            best_train_acc = train_acc
            # Save best model checkpoint
            torch.save({
                'epoch': epoch + 1,
                'model_state_dict': model.state_dict(),
                'optimizer_state_dict': optimizer.state_dict(),
                'scheduler_state_dict': scheduler.state_dict(),
                'train_loss': train_loss,
                'train_acc': train_acc,
                'best_train_acc': best_train_acc,
                'hyperparameters': {
                    'batch_size': batch_size,
                    'learning_rate': learning_rate,
                    'num_epochs': num_epochs,
                    'num_classes': num_classes
                }
            }, 'best_pointnet_modelnet40.pth')

        # Print progress
        print(f'Epoch [{epoch+1:3d}/{num_epochs}] | '
              f'Loss: {train_loss:.4f} | '
              f'Acc: {train_acc:.4f} | '
              f'Best Acc: {best_train_acc:.4f} | '
              f'LR: {current_lr:.6f} | '
              f'Time: {epoch_time:.2f}s')

        # Print progress bar every 10 epochs
        if (epoch + 1) % 10 == 0:
            progress = (epoch + 1) / num_epochs
            bar_length = 30
            filled_length = int(bar_length * progress)
            bar = '█' * filled_length + '-' * (bar_length - filled_length)
            print(f'Progress: |{bar}| {progress:.1%}')
            print("-" * 60)

        # Memory cleanup for GPU
        if torch.cuda.is_available():
            torch.cuda.empty_cache()

    print("\nTraining complete!")
    print(f"Best training accuracy achieved: {best_train_acc:.4f}")

    # Save final model
    final_model_path = 'pointnet_modelnet40_final.pth'
    torch.save({
        'epoch': num_epochs,
        'model_state_dict': model.state_dict(),
        'optimizer_state_dict': optimizer.state_dict(),
        'scheduler_state_dict': scheduler.state_dict(),
        'train_losses': train_losses,
        'train_accuracies': train_accuracies,
        'best_train_acc': best_train_acc,
        'final_train_acc': train_acc,
        'hyperparameters': {
            'batch_size': batch_size,
            'learning_rate': learning_rate,
            'num_epochs': num_epochs,
            'num_classes': num_classes
        }
    }, final_model_path)

    # Also save just the model state dict for easy loading
    torch.save(model.state_dict(), 'pointnet_modelnet40.pth')

    print(f"Final model saved to: {final_model_path}")
    print(f"Model state dict saved to: pointnet_modelnet40.pth")
    print(f"Best model checkpoint saved to: best_pointnet_modelnet40.pth")

    # Final model evaluation summary
    print("\n" + "="*60)
    print("TRAINING SUMMARY")
    print("="*60)
    print(f"Dataset: ModelNet40")
    print(f"Total epochs: {num_epochs}")
    print(f"Batch size: {batch_size}")
    print(f"Learning rate: {learning_rate}")
    print(f"Best training accuracy: {best_train_acc:.4f}")
    print(f"Final training accuracy: {train_acc:.4f}")
    print(f"Total parameters: {total_params:,}")
    print("="*60)

if __name__ == '__main__':
    main()

import torch
import torch.nn as nn
import torch.nn.functional as F
import h5py
import numpy as np
import os

# Check if GPU is available
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
print(f"Using device: {device}")

# Function to load test data from HDF5 with verification steps
def load_h5_data(h5_filename):
    """Load data from HDF5 file with proper error handling"""
    try:
        with h5py.File(h5_filename, 'r') as f:
            data = f['data'][:]
            labels = f['label'][:]
            print(f"Loaded {len(labels)} samples from {h5_filename}")
            print(f"Data shape: {data.shape}, Labels shape: {labels.shape}")
            # Print some sample labels to verify
            print(f"First 10 labels: {labels.squeeze()[:10]}")
        return data, labels
    except Exception as e:
        print(f"Error loading {h5_filename}: {e}")
        return None, None

# Locate test files
dataset_paths = ['/content/sample_data/Modelnet40']

# Find a valid dataset path
dataset_path = None
for path in dataset_paths:
    if os.path.exists(path):
        dataset_path = path
        print(f"Found dataset at: {dataset_path}")
        break

if not dataset_path:
    raise ValueError("Could not find ModelNet40 dataset")

# List all HDF5 files to ensure we're using separate test files
h5_files = [f for f in os.listdir(dataset_path) if f.endswith('.h5')]
print(f"Available H5 files: {h5_files}")

# Explicitly use test files (not train files)
test_files = [f for f in h5_files if 'test' in f.lower()]
if not test_files:
    print("No explicit test files found, looking for files that might be test data...")
    # If no explicit test files, try to use files that aren't training files
    non_train_files = [f for f in h5_files if 'train' not in f.lower()]
    if non_train_files:
        test_files = non_train_files
        print(f"Using non-training files as test files: {test_files}")
    else:
        # As a last resort, use any available file
        test_files = h5_files[:1]
        print(f"Using available file for testing: {test_files}")

if not test_files:
    raise ValueError("No suitable test files found in the dataset directory")

print(f"Using test files: {test_files}")

# Load test data from the first available test file
test_h5_file = os.path.join(dataset_path, test_files[0])
X_test, y_test = load_h5_data(test_h5_file)

if X_test is None or y_test is None:
    raise ValueError("Failed to load test data")

y_test = np.squeeze(y_test)  # Ensure labels are correctly shaped

# Verify input format before conversion
print(f"Input X shape before conversion: {X_test.shape}")
print(f"Input y shape before conversion: {y_test.shape}")
print(f"Label range: {y_test.min()} to {y_test.max()}")

# Convert to PyTorch tensors
# The data should be in format [N, num_points, 3]
if len(X_test.shape) == 3:
    if X_test.shape[2] == 3:
        # Data is already in [N, num_points, 3] format
        X_test_tensor = torch.tensor(X_test, dtype=torch.float32)
        print(f"Data format: [N, num_points, 3] = {X_test_tensor.shape}")
    elif X_test.shape[1] == 3:
        # Data is in [N, 3, num_points] format, transpose to [N, num_points, 3]
        X_test_tensor = torch.tensor(X_test.transpose(0, 2, 1), dtype=torch.float32)
        print(f"Data transposed from [N, 3, num_points] to [N, num_points, 3] = {X_test_tensor.shape}")
    else:
        raise ValueError(f"Unexpected data shape: {X_test.shape}")
else:
    raise ValueError(f"Expected 3D array, got shape: {X_test.shape}")

# For PointNet, we need [batch, channels, num_points] = [batch, 3, num_points]
X_test = X_test_tensor.permute(0, 2, 1)
print(f"Final tensor shape for PointNet [N, 3, num_points]: {X_test.shape}")

y_test = torch.tensor(y_test, dtype=torch.long)
print(f"Labels tensor shape: {y_test.shape}")

# Load trained model
num_classes = 40
print("Initializing model...")
model = PointNetClassification(num_classes=num_classes, feature_transform=False).to(device)

# Try to load the model with proper error handling
model_paths = [
    # "pointnet_modelnet40.pth",
    # "best_pointnet_modelnet40.pth",
    "pointnet_modelnet40_final.pth"
]

model_loaded = False
for model_path in model_paths:
    if os.path.exists(model_path):
        try:
            if model_path in ["best_pointnet_modelnet40.pth", "pointnet_modelnet40_final.pth"]:
                # These are full checkpoints
                checkpoint = torch.load(model_path, map_location=device)
                model.load_state_dict(checkpoint['model_state_dict'])
                print(f"Loaded model from checkpoint: {model_path}")
                if 'best_train_acc' in checkpoint:
                    print(f"Best training accuracy from checkpoint: {checkpoint['best_train_acc']:.4f}")
            else:
                # This is just the state dict
                model.load_state_dict(torch.load(model_path, map_location=device))
                print(f"Loaded model state dict from: {model_path}")
            model_loaded = True
            break
        except Exception as e:
            print(f"Failed to load {model_path}: {e}")
            continue

if not model_loaded:
    raise ValueError("Could not load any model file. Make sure you have trained the model first.")

model.eval()  # Set model to evaluation mode
print("Model set to evaluation mode")

# Print model architecture for verification
print("\nModel architecture:")
print(model)

# Perform testing with batch processing for efficiency
batch_size = 32  # Process in batches to manage memory
correct = 0
total = len(y_test)
class_correct = [0] * num_classes
class_total = [0] * num_classes

print(f"\nStarting evaluation on {total} samples...")
print("="*60)

with torch.no_grad():
    for batch_start in range(0, total, batch_size):
        batch_end = min(batch_start + batch_size, total)

        # Get batch data
        batch_X = X_test[batch_start:batch_end].to(device)
        batch_y = y_test[batch_start:batch_end].to(device)

        # Forward pass
        outputs = model(batch_X)

        # Get predictions
        _, predicted = torch.max(outputs.data, 1)

        # Track correctness for each sample in batch
        for i in range(len(batch_y)):
            label = batch_y[i].item()
            pred = predicted[i].item()

            if pred == label:
                correct += 1
                class_correct[label] += 1
            class_total[label] += 1

            # Print detailed info for first few samples
            sample_idx = batch_start + i
            if sample_idx < 10:
                print(f"Sample {sample_idx}: Label={label}, Prediction={pred}, Correct={pred==label}")

        # Print progress
        if (batch_end) % (batch_size * 5) == 0 or batch_end == total:
            current_acc = correct / batch_end
            print(f"Processed {batch_end}/{total} samples. Current accuracy: {current_acc:.4f}")

# Compute and print final accuracy
final_accuracy = correct / total
print("\n" + "="*60)
print("EVALUATION RESULTS")
print("="*60)
print(f"Overall Test Accuracy: {final_accuracy:.4f} ({correct}/{total})")
print(f"Error Rate: {(1-final_accuracy):.4f}")

# Print per-class accuracy
print(f"\nPer-class accuracy (showing first 20 classes):")
print("-" * 40)
for i in range(min(20, num_classes)):
    if class_total[i] > 0:
        class_acc = class_correct[i] / class_total[i]
        print(f"Class {i:2d}: {class_acc:.4f} ({class_correct[i]:3d}/{class_total[i]:3d})")
    else:
        print(f"Class {i:2d}: No samples")

# Additional statistics
print(f"\nAdditional Statistics:")
print("-" * 40)
print(f"Total classes with samples: {sum(1 for x in class_total if x > 0)}")
print(f"Average samples per class: {total/num_classes:.1f}")
print(f"Min samples in a class: {min(x for x in class_total if x > 0) if any(x > 0 for x in class_total) else 0}")
print(f"Max samples in a class: {max(class_total)}")

# Memory cleanup
if torch.cuda.is_available():
    torch.cuda.empty_cache()
    print(f"\nGPU memory cleaned up")

print("="*60)
print("Evaluation completed successfully!")
print("="*60)



